{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### MOVIES DATA EXTRACTION:###"
      ],
      "metadata": {
        "id": "VW-_Q5vzdoWd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9h0cknvE0tp",
        "outputId": "5bf651f7-d791-4b13-c1d7-3b480a703c06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tmdbsimple\n",
            "  Obtaining dependency information for tmdbsimple from https://files.pythonhosted.org/packages/6c/dd/ade05d202db728b23e54aa0959622d090776023917e7308c1b2469a07b76/tmdbsimple-2.9.1-py3-none-any.whl.metadata\n",
            "  Downloading tmdbsimple-2.9.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: requests in /Users/pilar/anaconda3/lib/python3.11/site-packages (from tmdbsimple) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pilar/anaconda3/lib/python3.11/site-packages (from requests->tmdbsimple) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/pilar/anaconda3/lib/python3.11/site-packages (from requests->tmdbsimple) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pilar/anaconda3/lib/python3.11/site-packages (from requests->tmdbsimple) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/pilar/anaconda3/lib/python3.11/site-packages (from requests->tmdbsimple) (2024.8.30)\n",
            "Downloading tmdbsimple-2.9.1-py3-none-any.whl (38 kB)\n",
            "Installing collected packages: tmdbsimple\n",
            "Successfully installed tmdbsimple-2.9.1\n"
          ]
        }
      ],
      "source": [
        "#!pip install tmdbsimple\n",
        "\n",
        "import requests\n",
        "import tmdbsimple as tmdb\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-suY5KMdjlB"
      },
      "source": [
        " Check what's the movie info that we can download per movie observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuqV_gfKdjlC",
        "outputId": "6242a5cd-2991-4104-ea93-041eb97a45e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available movie fields (columns):\n",
            "adult\n",
            "backdrop_path\n",
            "belongs_to_collection\n",
            "budget\n",
            "genres\n",
            "homepage\n",
            "id\n",
            "imdb_id\n",
            "origin_country\n",
            "original_language\n",
            "original_title\n",
            "overview\n",
            "popularity\n",
            "poster_path\n",
            "production_companies\n",
            "production_countries\n",
            "release_date\n",
            "revenue\n",
            "runtime\n",
            "spoken_languages\n",
            "status\n",
            "tagline\n",
            "title\n",
            "video\n",
            "vote_average\n",
            "vote_count\n"
          ]
        }
      ],
      "source": [
        "import tmdbsimple as tmdb\n",
        "\n",
        "# Replace with your actual TMDB API key\n",
        "tmdb.API_KEY = '8db08b812689d86a7bf90611bf5a1eee'\n",
        "\n",
        "# Use an example movie ID (550 is Fight Club)\n",
        "movie_id = 550\n",
        "\n",
        "movie = tmdb.Movies(movie_id)\n",
        "data = movie.info()\n",
        "\n",
        "# Print out all the available keys in the movie details response\n",
        "print(\"Available movie fields (columns):\")\n",
        "for key in data:\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0fZEhPzdjlD"
      },
      "source": [
        "Some of the fields (like genres, production companies, spoken languages) are nested structures. This will require additional code to explore and flatten the nested keys to have every subfield as an addittional column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mSwUHyyE8CM",
        "outputId": "347622d0-266b-419c-c2e2-3b2be713c7ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching page 1 of movies...\n",
            "Written movie: The Gorge\n",
            "Written movie: Mufasa: The Lion King\n",
            "Written movie: Flight Risk\n",
            "Written movie: Moana 2\n",
            "Written movie: Sonic the Hedgehog 3\n",
            "Written movie: Amaran\n",
            "Written movie: Captain America: Brave New World\n",
            "Written movie: Panda Plan\n",
            "Written movie: Companion\n",
            "Written movie: My Fault: London\n",
            "Written movie: Kraven the Hunter\n",
            "Written movie: Paddington in Peru\n",
            "Written movie: Dog Man\n",
            "Written movie: Death Whisperer 2\n",
            "Written movie: The Island\n",
            "Written movie: Alarum\n",
            "Written movie: Venom: The Last Dance\n",
            "Written movie: The Brutalist\n",
            "Written movie: Gladiator II\n",
            "Written movie: Sniper: The Last Stand\n",
            "Fetching page 2 of movies...\n",
            "Written movie: Back in Action\n",
            "Written movie: Dhoom Dhaam\n",
            "Written movie: Ball Red Daughter-in-law\n",
            "Written movie: Devara: Part 1\n",
            "Written movie: Wolf Man\n",
            "Written movie: Elevation\n",
            "Written movie: The Gardener\n",
            "Written movie: Solo Leveling -ReAwakening-\n",
            "Written movie: Le clitoris\n",
            "Written movie: Star Trek: Section 31\n",
            "Written movie: Nosferatu\n",
            "Written movie: My Fault\n",
            "Written movie: Piglet\n",
            "Written movie: Werewolves\n",
            "Written movie: Red One\n",
            "Written movie: Deadpool & Wolverine\n",
            "Written movie: The Substance\n",
            "Written movie: Bridget Jones: Mad About the Boy\n",
            "Written movie: Despicable Me 4\n",
            "Written movie: The Lord of the Rings: The War of the Rohirrim\n",
            "Fetching page 3 of movies...\n",
            "Written movie: The Wild Robot\n",
            "Written movie: Flow\n",
            "Written movie: The Wailing\n",
            "Written movie: Jugaremos en el bosque\n",
            "Written movie: Chasing the Wind\n",
            "Written movie: Your Fault\n",
            "Written movie: The Witcher: Sirens of the Deep\n",
            "Written movie: Absolution\n",
            "Written movie: Memoir of a Snail\n",
            "Written movie: Anora\n",
            "Written movie: Raiders of the Lost Ark\n",
            "Written movie: Ask Me What You Want\n",
            "Written movie: Aftermath\n",
            "Written movie: Dirty Angels\n",
            "Written movie: Inside Out 2\n",
            "Written movie: Belyas\n",
            "Written movie: Ne Zha 2\n",
            "Written movie: Number 24\n",
            "Written movie: Sex Game 6969\n",
            "Written movie: Wicked\n",
            "Fetching page 4 of movies...\n",
            "Written movie: The Monkey\n",
            "Written movie: Alien: Romulus\n",
            "Written movie: Star Trek: Section 31\n",
            "Written movie: Elyas\n",
            "Written movie: Babygirl\n",
            "Written movie: Nosferatu\n",
            "Written movie: Jurassic World Dominion\n",
            "Written movie: Love Forever\n",
            "Written movie: Transformers One\n",
            "Written movie: The Chronicles of Narnia: The Lion, the Witch and the Wardrobe\n",
            "Written movie: Eye for an Eye 2\n",
            "Written movie: Miraculous World, London: At the Edge of Time\n",
            "Written movie: Heretic\n",
            "Written movie: Conclave\n",
            "Written movie: Azrael\n",
            "Written movie: Smile 2\n",
            "Written movie: Straight\n",
            "Written movie: Avengers: Infinity War\n",
            "Written movie: Terrifier 3\n",
            "Written movie: Carry-On\n",
            "Fetching page 5 of movies...\n",
            "Written movie: The Lord of the Rings: The War of the Rohirrim\n",
            "Written movie: Interstellar\n",
            "Written movie: Tabu\n",
            "Written movie: The Ballad of Davy Crockett\n",
            "Written movie: Chasing the Wind\n",
            "Written movie: Undercover\n",
            "Written movie: Big World\n",
            "Written movie: 365 Days: This Day\n",
            "Written movie: Sumala\n",
            "Written movie: The Garfield Movie\n",
            "Written movie: La Dolce Villa\n",
            "Written movie: Survive\n",
            "Written movie: The Lion King\n",
            "Written movie: Sister-in-law's Taste 2\n",
            "Written movie: This Time Next Year\n",
            "Written movie: Wallace & Gromit: Vengeance Most Fowl\n",
            "Written movie: Ne Zha 2\n",
            "Written movie: xXx\n",
            "Written movie: Spider-Man: No Way Home\n",
            "Written movie: The Avengers\n"
          ]
        }
      ],
      "source": [
        "### TRIAL HOW TO DOWNLOAD MOVIES\n",
        "\n",
        "import csv\n",
        "import tmdbsimple as tmdb\n",
        "\n",
        "\n",
        "def fetch_movie_data(movie_id):\n",
        "    \"\"\"Fetch detailed movie info and credits for a given movie ID.\"\"\"\n",
        "    movie = tmdb.Movies(movie_id)\n",
        "    try:\n",
        "        details = movie.info()\n",
        "        credits = movie.credits()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for movie ID {movie_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Extract genres and join into a comma-separated string\n",
        "    genres = [genre['name'] for genre in details.get('genres', [])]\n",
        "    genres_str = \", \".join(genres)\n",
        "\n",
        "    # Extract the top 5 cast members and join their names\n",
        "    cast_list = credits.get('cast', [])\n",
        "    cast_names = [member['name'] for member in cast_list[:5]]\n",
        "    cast_str = \", \".join(cast_names)\n",
        "\n",
        "    return {\n",
        "        'movie_id': details.get('id'),\n",
        "        'title': details.get('title'),\n",
        "        'release_date': details.get('release_date'),\n",
        "        'genres': genres_str,\n",
        "        'cast': cast_str,\n",
        "        'revenue': details.get('revenue'),\n",
        "        'budget': details.get('budget'),\n",
        "        'runtime': details.get('runtime'),\n",
        "        'original_language': details.get('original_language'),\n",
        "        'popularity': details.get('popularity'),\n",
        "        'vote_average': details.get('vote_average'),\n",
        "        'vote_count': details.get('vote_count'),\n",
        "        'overview': details.get('overview')\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    output_file = 'tmdb_movies.csv'\n",
        "    fieldnames = [\n",
        "        'movie_id', 'title', 'release_date', 'genres', 'cast',\n",
        "        'revenue', 'budget', 'runtime', 'original_language',\n",
        "        'popularity', 'vote_average', 'vote_count', 'overview'\n",
        "    ]\n",
        "\n",
        "    with open(output_file, mode='w', newline='', encoding='utf-8') as csv_file:\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        discover = tmdb.Discover()\n",
        "        # Adjust total_pages as needed to gather more movies.\n",
        "        total_pages = 5\n",
        "        for page in range(1, total_pages + 1):\n",
        "            print(f\"Fetching page {page} of movies...\")\n",
        "            try:\n",
        "                discover_response = discover.movie(page=page)\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching discover page {page}: {e}\")\n",
        "                continue\n",
        "\n",
        "            movies = discover_response.get('results', [])\n",
        "            for movie in movies:\n",
        "                movie_id = movie.get('id')\n",
        "                movie_data = fetch_movie_data(movie_id)\n",
        "                if movie_data:\n",
        "                    writer.writerow(movie_data)\n",
        "                    print(f\"Written movie: {movie_data.get('title')}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1p3TPDddjlE"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GskCux1tdjlE"
      },
      "source": [
        "\n",
        "\n",
        "When designing your database schema, it's helpful to normalize your data by moving multi-valued and relational attributes into separate tables. Based on the TMDB movie details structure, here are some recommendations:\n",
        "\n",
        "Genres:\n",
        "Movies often belong to multiple genres. Create a separate Genres table and a join table (e.g., MovieGenres) to relate each movie to its genres.\n",
        "\n",
        "Production Companies:\n",
        "A movie can involve several production companies, and the same company might produce many movies. Having a ProductionCompanies table (plus a join table like MovieProductionCompanies) makes it easier to query and update company information without duplicating data.\n",
        "\n",
        "Production Countries:\n",
        "Similarly, since movies can be associated with multiple countries, a ProductionCountries table with a join table can help manage this many-to-many relationship.\n",
        "\n",
        "Spoken Languages:\n",
        "Movies can feature several spoken languages. A dedicated SpokenLanguages table with a linking table (e.g., MovieLanguages) keeps this relationship normalized.\n",
        "\n",
        "Collections (Belongs_to_collection):\n",
        "If you want to explore movie series or collections in more detail, a separate Collections table can be useful. This table can store additional information about the collection beyond just the name.\n",
        "\n",
        "Cast and Crew (if extended):\n",
        "Although not in your current list, if you decide to include cast/crew details later, these should definitely reside in separate tables (like Cast and Crew with their respective join tables) because a single movie can have many cast members, and each actor might appear in multiple movies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDT3seAbdjlE"
      },
      "outputs": [],
      "source": [
        "## TMDBâ€™s Discover endpoint typically limits results to 500 pages. so better filter by dates first to handle each section of movies individually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aAgBIvQdjlF"
      },
      "outputs": [],
      "source": [
        "# trial only extract first 100 movies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpKMf8l0djlF",
        "outputId": "639c7b13-b206-4f61-882c-26f953ebf6c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing movies released from 1900-01-01 to 1950-12-31\n",
            "  Total pages in this range: 500\n",
            "  Processing page 1/500 for range 1900-01-01 to 1950-12-31...\n",
            "  Processed 1 movies so far.\n",
            "  Processed 2 movies so far.\n",
            "  Processed 3 movies so far.\n",
            "  Processed 4 movies so far.\n",
            "  Processed 5 movies so far.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 234\u001b[0m\n\u001b[1;32m    231\u001b[0m                     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Exit page loop if limit reached\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 234\u001b[0m     main()\n",
            "Cell \u001b[0;32mIn[21], line 201\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Exit inner loop if limit reached\u001b[39;00m\n\u001b[1;32m    200\u001b[0m movie_id \u001b[38;5;241m=\u001b[39m movie\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 201\u001b[0m details \u001b[38;5;241m=\u001b[39m get_movie_details(movie_id)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m details \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[21], line 15\u001b[0m, in \u001b[0;36mget_movie_details\u001b[0;34m(movie_id)\u001b[0m\n\u001b[1;32m     13\u001b[0m movie \u001b[38;5;241m=\u001b[39m tmdb\u001b[38;5;241m.\u001b[39mMovies(movie_id)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     details \u001b[38;5;241m=\u001b[39m movie\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m details\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tmdbsimple/movies.py:73\u001b[0m, in \u001b[0;36mMovies.info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03mGet the primary information about a movie.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    A dict representation of the JSON returned from the API.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_id_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_GET(path, kwargs)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_attrs_to_values(response)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tmdbsimple/base.py:110\u001b[0m, in \u001b[0;36mTMDB._GET\u001b[0;34m(self, path, params)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_GET\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, path, params\u001b[38;5;241m=\u001b[39mparams)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tmdbsimple/base.py:87\u001b[0m, in \u001b[0;36mTMDB._request\u001b[0;34m(self, method, path, params, payload)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Create a new request session if no global session is defined\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m     88\u001b[0m         method,\n\u001b[1;32m     89\u001b[0m         url,\n\u001b[1;32m     90\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m     91\u001b[0m         data\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(payload) \u001b[38;5;28;01mif\u001b[39;00m payload \u001b[38;5;28;01melse\u001b[39;00m payload,\n\u001b[1;32m     92\u001b[0m         headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Use the global requests session the user provided\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m     98\u001b[0m         method,\n\u001b[1;32m     99\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m         headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\n\u001b[1;32m    103\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import time\n",
        "import tmdbsimple as tmdb\n",
        "\n",
        "# Set your TMDB API key\n",
        "tmdb.API_KEY = '8db08b812689d86a7bf90611bf5a1eee'  # Replace with your actual API key\n",
        "\n",
        "# -----------------------------\n",
        "# Data Processing Functions\n",
        "# -----------------------------\n",
        "def get_movie_details(movie_id):\n",
        "    \"\"\"Fetch detailed information for a movie by its ID.\"\"\"\n",
        "    movie = tmdb.Movies(movie_id)\n",
        "    try:\n",
        "        details = movie.info()\n",
        "        return details\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving movie {movie_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_main_data(details):\n",
        "    \"\"\"Extract single-valued (non-normalized) fields for the main movies table.\"\"\"\n",
        "    return {\n",
        "        \"id\": details.get(\"id\"),\n",
        "        \"adult\": details.get(\"adult\"),\n",
        "        \"backdrop_path\": details.get(\"backdrop_path\"),\n",
        "        \"budget\": details.get(\"budget\"),\n",
        "        \"homepage\": details.get(\"homepage\"),\n",
        "        \"imdb_id\": details.get(\"imdb_id\"),\n",
        "        \"original_language\": details.get(\"original_language\"),\n",
        "        \"original_title\": details.get(\"original_title\"),\n",
        "        \"overview\": details.get(\"overview\"),\n",
        "        \"popularity\": details.get(\"popularity\"),\n",
        "        \"poster_path\": details.get(\"poster_path\"),\n",
        "        \"release_date\": details.get(\"release_date\"),\n",
        "        \"revenue\": details.get(\"revenue\"),\n",
        "        \"runtime\": details.get(\"runtime\"),\n",
        "        \"status\": details.get(\"status\"),\n",
        "        \"tagline\": details.get(\"tagline\"),\n",
        "        \"title\": details.get(\"title\"),\n",
        "        \"video\": details.get(\"video\"),\n",
        "        \"vote_average\": details.get(\"vote_average\"),\n",
        "        \"vote_count\": details.get(\"vote_count\")\n",
        "    }\n",
        "\n",
        "def process_collection(details):\n",
        "    \"\"\"Extract collection info (belongs_to_collection) into its own record.\"\"\"\n",
        "    coll = details.get(\"belongs_to_collection\")\n",
        "    if coll:\n",
        "        return {\n",
        "            \"movie_id\": details.get(\"id\"),\n",
        "            \"collection_id\": coll.get(\"id\"),\n",
        "            \"collection_name\": coll.get(\"name\")\n",
        "        }\n",
        "    return None\n",
        "\n",
        "def process_genres(details):\n",
        "    \"\"\"Extract the list of genres for a movie.\"\"\"\n",
        "    genres_list = details.get(\"genres\", [])\n",
        "    rows = []\n",
        "    movie_id = details.get(\"id\")\n",
        "    for genre in genres_list:\n",
        "        rows.append({\n",
        "            \"movie_id\": movie_id,\n",
        "            \"genre_id\": genre.get(\"id\"),\n",
        "            \"genre_name\": genre.get(\"name\")\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "def process_production_companies(details):\n",
        "    \"\"\"Extract production companies for a movie.\"\"\"\n",
        "    companies = details.get(\"production_companies\", [])\n",
        "    rows = []\n",
        "    movie_id = details.get(\"id\")\n",
        "    for comp in companies:\n",
        "        rows.append({\n",
        "            \"movie_id\": movie_id,\n",
        "            \"company_id\": comp.get(\"id\"),\n",
        "            \"company_name\": comp.get(\"name\"),\n",
        "            \"origin_country\": comp.get(\"origin_country\")\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "def process_production_countries(details):\n",
        "    \"\"\"Extract production countries for a movie.\"\"\"\n",
        "    countries = details.get(\"production_countries\", [])\n",
        "    rows = []\n",
        "    movie_id = details.get(\"id\")\n",
        "    for country in countries:\n",
        "        rows.append({\n",
        "            \"movie_id\": movie_id,\n",
        "            \"iso_3166_1\": country.get(\"iso_3166_1\"),\n",
        "            \"country_name\": country.get(\"name\")\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "def process_spoken_languages(details):\n",
        "    \"\"\"Extract spoken languages for a movie.\"\"\"\n",
        "    languages = details.get(\"spoken_languages\", [])\n",
        "    rows = []\n",
        "    movie_id = details.get(\"id\")\n",
        "    for lang in languages:\n",
        "        rows.append({\n",
        "            \"movie_id\": movie_id,\n",
        "            \"iso_639_1\": lang.get(\"iso_639_1\"),\n",
        "            \"language_name\": lang.get(\"name\")\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "# -----------------------------\n",
        "# Main ETL Process\n",
        "# -----------------------------\n",
        "def main():\n",
        "    # Define output CSV filenames\n",
        "    main_file = \"tmdb_movies_main.csv\"\n",
        "    collection_file = \"tmdb_movie_collection.csv\"\n",
        "    genres_file = \"tmdb_movie_genres.csv\"\n",
        "    production_companies_file = \"tmdb_movie_production_companies.csv\"\n",
        "    production_countries_file = \"tmdb_movie_production_countries.csv\"\n",
        "    spoken_languages_file = \"tmdb_movie_spoken_languages.csv\"\n",
        "\n",
        "    # Define CSV headers for each file\n",
        "    main_headers = [\n",
        "        \"id\", \"adult\", \"backdrop_path\", \"budget\", \"homepage\", \"imdb_id\",\n",
        "        \"original_language\", \"original_title\", \"overview\", \"popularity\",\n",
        "        \"poster_path\", \"release_date\", \"revenue\", \"runtime\", \"status\",\n",
        "        \"tagline\", \"title\", \"video\", \"vote_average\", \"vote_count\"\n",
        "    ]\n",
        "    collection_headers = [\"movie_id\", \"collection_id\", \"collection_name\"]\n",
        "    genres_headers = [\"movie_id\", \"genre_id\", \"genre_name\"]\n",
        "    production_companies_headers = [\"movie_id\", \"company_id\", \"company_name\", \"origin_country\"]\n",
        "    production_countries_headers = [\"movie_id\", \"iso_3166_1\", \"country_name\"]\n",
        "    spoken_languages_headers = [\"movie_id\", \"iso_639_1\", \"language_name\"]\n",
        "\n",
        "    # Open CSV files for writing\n",
        "    with open(main_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as main_csv, \\\n",
        "         open(collection_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as coll_csv, \\\n",
        "         open(genres_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as genres_csv, \\\n",
        "         open(production_companies_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as prod_comp_csv, \\\n",
        "         open(production_countries_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as prod_countries_csv, \\\n",
        "         open(spoken_languages_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as spoken_csv:\n",
        "\n",
        "        main_writer = csv.DictWriter(main_csv, fieldnames=main_headers)\n",
        "        coll_writer = csv.DictWriter(coll_csv, fieldnames=collection_headers)\n",
        "        genres_writer = csv.DictWriter(genres_csv, fieldnames=genres_headers)\n",
        "        prod_comp_writer = csv.DictWriter(prod_comp_csv, fieldnames=production_companies_headers)\n",
        "        prod_countries_writer = csv.DictWriter(prod_countries_csv, fieldnames=production_countries_headers)\n",
        "        spoken_writer = csv.DictWriter(spoken_csv, fieldnames=spoken_languages_headers)\n",
        "\n",
        "        # Write headers to each CSV file\n",
        "        main_writer.writeheader()\n",
        "        coll_writer.writeheader()\n",
        "        genres_writer.writeheader()\n",
        "        prod_comp_writer.writeheader()\n",
        "        prod_countries_writer.writeheader()\n",
        "        spoken_writer.writeheader()\n",
        "\n",
        "        # We'll use only one date range to simplify and then only process 100 movies.\n",
        "        date_ranges = [\n",
        "            {\"gte\": \"1900-01-01\", \"lte\": \"1950-12-31\"}\n",
        "        ]\n",
        "\n",
        "        movies_processed = 0  # Counter for processed movies\n",
        "\n",
        "        # Loop through each date range partition\n",
        "        for dr in date_ranges:\n",
        "            print(f\"Processing movies released from {dr['gte']} to {dr['lte']}\")\n",
        "            discover = tmdb.Discover()\n",
        "            params = {\n",
        "                \"page\": 1,\n",
        "                \"primary_release_date.gte\": dr[\"gte\"],\n",
        "                \"primary_release_date.lte\": dr[\"lte\"]\n",
        "            }\n",
        "            try:\n",
        "                first_page_response = discover.movie(**params)\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching first page for date range {dr}: {e}\")\n",
        "                continue\n",
        "\n",
        "            total_pages = first_page_response.get(\"total_pages\", 1)\n",
        "            if total_pages > 500:\n",
        "                total_pages = 500\n",
        "            print(f\"  Total pages in this range: {total_pages}\")\n",
        "\n",
        "            # Process each page in this date range\n",
        "            for page in range(1, total_pages + 1):\n",
        "                if movies_processed >= 100:\n",
        "                    break  # Exit if we've processed 100 movies\n",
        "                params[\"page\"] = page\n",
        "                print(f\"  Processing page {page}/{total_pages} for range {dr['gte']} to {dr['lte']}...\")\n",
        "                try:\n",
        "                    response = discover.movie(**params)\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error on page {page}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                for movie in response.get(\"results\", []):\n",
        "                    if movies_processed >= 100:\n",
        "                        break  # Exit inner loop if limit reached\n",
        "                    movie_id = movie.get(\"id\")\n",
        "                    details = get_movie_details(movie_id)\n",
        "                    if details is None:\n",
        "                        continue\n",
        "\n",
        "                    # Write single-valued movie data\n",
        "                    main_data = process_main_data(details)\n",
        "                    main_writer.writerow(main_data)\n",
        "\n",
        "                    # Write collection data (if any)\n",
        "                    collection_data = process_collection(details)\n",
        "                    if collection_data:\n",
        "                        coll_writer.writerow(collection_data)\n",
        "\n",
        "                    # Write multi-valued data into their respective CSV files\n",
        "                    for row in process_genres(details):\n",
        "                        genres_writer.writerow(row)\n",
        "                    for row in process_production_companies(details):\n",
        "                        prod_comp_writer.writerow(row)\n",
        "                    for row in process_production_countries(details):\n",
        "                        prod_countries_writer.writerow(row)\n",
        "                    for row in process_spoken_languages(details):\n",
        "                        spoken_writer.writerow(row)\n",
        "\n",
        "                    movies_processed += 1\n",
        "                    print(f\"  Processed {movies_processed} movies so far.\")\n",
        "\n",
        "                    # Pause briefly to respect API rate limits\n",
        "                    time.sleep(0.25)\n",
        "\n",
        "                if movies_processed >= 100:\n",
        "                    break  # Exit page loop if limit reached\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5pHpwbjdjlG"
      },
      "source": [
        " parallelise with threadpool executors?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBUjLcqmdjlG"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import tmdbsimple as tmdb\n",
        "import threading\n",
        "import concurrent.futures\n",
        "\n",
        "tmdb.API_KEY = '8db08b812689d86a7bf90611bf5a1eee'\n",
        "\n",
        "# -----------------------------\n",
        "# Data Processing Functions\n",
        "# -----------------------------\n",
        "def get_movie_details(movie_id):\n",
        "    \"\"\"Fetch detailed information for a movie by its ID.\"\"\"\n",
        "    movie = tmdb.Movies(movie_id)\n",
        "    try:\n",
        "        details = movie.info()\n",
        "        return details\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving movie {movie_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_main_data(details):\n",
        "    \"\"\"Extract single-valued (non-normalized) fields for the main movies table.\"\"\"\n",
        "    return {\n",
        "        \"id\": details.get(\"id\"),\n",
        "        \"adult\": details.get(\"adult\"),\n",
        "        \"backdrop_path\": details.get(\"backdrop_path\"),\n",
        "        \"budget\": details.get(\"budget\"),\n",
        "        \"homepage\": details.get(\"homepage\"),\n",
        "        \"imdb_id\": details.get(\"imdb_id\"),\n",
        "        \"original_language\": details.get(\"original_language\"),\n",
        "        \"original_title\": details.get(\"original_title\"),\n",
        "        \"overview\": details.get(\"overview\"),\n",
        "        \"popularity\": details.get(\"popularity\"),\n",
        "        \"poster_path\": details.get(\"poster_path\"),\n",
        "        \"release_date\": details.get(\"release_date\"),\n",
        "        \"revenue\": details.get(\"revenue\"),\n",
        "        \"runtime\": details.get(\"runtime\"),\n",
        "        \"status\": details.get(\"status\"),\n",
        "        \"tagline\": details.get(\"tagline\"),\n",
        "        \"title\": details.get(\"title\"),\n",
        "        \"video\": details.get(\"video\"),\n",
        "        \"vote_average\": details.get(\"vote_average\"),\n",
        "        \"vote_count\": details.get(\"vote_count\")\n",
        "    }\n",
        "\n",
        "def process_collection(details):\n",
        "    \"\"\"Extract collection info (belongs_to_collection) into its own record.\"\"\"\n",
        "    coll = details.get(\"belongs_to_collection\")\n",
        "    if coll:\n",
        "        return {\n",
        "            \"movie_id\": details.get(\"id\"),\n",
        "            \"collection_id\": coll.get(\"id\"),\n",
        "            \"collection_name\": coll.get(\"name\")\n",
        "        }\n",
        "    return None\n",
        "\n",
        "def process_genres(details):\n",
        "    \"\"\"Extract the list of genres for a movie.\"\"\"\n",
        "    genres_list = details.get(\"genres\", [])\n",
        "    rows = []\n",
        "    movie_id = details.get(\"id\")\n",
        "    for genre in genres_list:\n",
        "        rows.append({\n",
        "            \"movie_id\": movie_id,\n",
        "            \"genre_id\": genre.get(\"id\"),\n",
        "            \"genre_name\": genre.get(\"name\")\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "def process_production_companies(details):\n",
        "    \"\"\"Extract production companies for a movie.\"\"\"\n",
        "    companies = details.get(\"production_companies\", [])\n",
        "    rows = []\n",
        "    movie_id = details.get(\"id\")\n",
        "    for comp in companies:\n",
        "        rows.append({\n",
        "            \"movie_id\": movie_id,\n",
        "            \"company_id\": comp.get(\"id\"),\n",
        "            \"company_name\": comp.get(\"name\"),\n",
        "            \"origin_country\": comp.get(\"origin_country\")\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "def process_production_countries(details):\n",
        "    \"\"\"Extract production countries for a movie.\"\"\"\n",
        "    countries = details.get(\"production_countries\", [])\n",
        "    rows = []\n",
        "    movie_id = details.get(\"id\")\n",
        "    for country in countries:\n",
        "        rows.append({\n",
        "            \"movie_id\": movie_id,\n",
        "            \"iso_3166_1\": country.get(\"iso_3166_1\"),\n",
        "            \"country_name\": country.get(\"name\")\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "def process_spoken_languages(details):\n",
        "    \"\"\"Extract spoken languages for a movie.\"\"\"\n",
        "    languages = details.get(\"spoken_languages\", [])\n",
        "    rows = []\n",
        "    movie_id = details.get(\"id\")\n",
        "    for lang in languages:\n",
        "        rows.append({\n",
        "            \"movie_id\": movie_id,\n",
        "            \"iso_639_1\": lang.get(\"iso_639_1\"),\n",
        "            \"language_name\": lang.get(\"name\")\n",
        "        })\n",
        "    return rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frjawmfodjlG"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Helper: Process a Single Movie\n",
        "# -----------------------------\n",
        "def process_movie(movie, main_writer, coll_writer, genres_writer,\n",
        "                  prod_comp_writer, prod_countries_writer, spoken_writer, lock):\n",
        "    \"\"\"\n",
        "    Process a single movie: fetch details, write data to CSV (within a lock),\n",
        "    and then sleep to help respect rate limits.\n",
        "    \"\"\"\n",
        "    movie_id = movie.get(\"id\")\n",
        "    details = get_movie_details(movie_id)\n",
        "    if details is None:\n",
        "        return\n",
        "    with lock:\n",
        "        main_data = process_main_data(details)\n",
        "        main_writer.writerow(main_data)\n",
        "\n",
        "        collection_data = process_collection(details)\n",
        "        if collection_data:\n",
        "            coll_writer.writerow(collection_data)\n",
        "\n",
        "        for row in process_genres(details):\n",
        "            genres_writer.writerow(row)\n",
        "        for row in process_production_companies(details):\n",
        "            prod_comp_writer.writerow(row)\n",
        "        for row in process_production_countries(details):\n",
        "            prod_countries_writer.writerow(row)\n",
        "        for row in process_spoken_languages(details):\n",
        "            spoken_writer.writerow(row)\n",
        "    # Sleep to respect API rate limits.\n",
        "    time.sleep(0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVRjig45djlH"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Recursive Date Range Processing with Parallelization\n",
        "# -----------------------------\n",
        "def process_date_range(gte, lte, main_writer, coll_writer, genres_writer,\n",
        "                       prod_comp_writer, prod_countries_writer, spoken_writer,\n",
        "                       flush_files, lock):\n",
        "    \"\"\"\n",
        "    Process movies released between gte and lte.\n",
        "    If the TMDB discover query returns 500 or more pages, split the range.\n",
        "    For each page, process movies in parallel and flush file buffers after every 500 pages.\n",
        "    \"\"\"\n",
        "    print(f\"\\nProcessing movies released from {gte} to {lte}\")\n",
        "    discover = tmdb.Discover()\n",
        "    params = {\n",
        "        \"page\": 1,\n",
        "        \"primary_release_date.gte\": gte,\n",
        "        \"primary_release_date.lte\": lte\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        first_page_response = discover.movie(**params)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching first page for range {gte} to {lte}: {e}\")\n",
        "        return\n",
        "\n",
        "    total_pages = first_page_response.get(\"total_pages\", 1)\n",
        "    print(f\"  Total pages in this range: {total_pages}\")\n",
        "\n",
        "    # If we hit the 500-page cap, split the range.\n",
        "    if total_pages >= 500:\n",
        "        start_date = datetime.datetime.strptime(gte, \"%Y-%m-%d\")\n",
        "        end_date = datetime.datetime.strptime(lte, \"%Y-%m-%d\")\n",
        "        if start_date >= end_date:\n",
        "            print(\"  Date range too narrow to split further. Processing with current cap.\")\n",
        "        else:\n",
        "            mid_date = start_date + (end_date - start_date) / 2\n",
        "            mid_str = mid_date.strftime(\"%Y-%m-%d\")\n",
        "            print(f\"  Splitting range: {gte} to {mid_str} and {mid_str} to {lte}\")\n",
        "            process_date_range(gte, mid_str, main_writer, coll_writer, genres_writer,\n",
        "                               prod_comp_writer, prod_countries_writer, spoken_writer,\n",
        "                               flush_files, lock)\n",
        "            process_date_range(mid_str, lte, main_writer, coll_writer, genres_writer,\n",
        "                               prod_comp_writer, prod_countries_writer, spoken_writer,\n",
        "                               flush_files, lock)\n",
        "            return\n",
        "\n",
        "    pages_processed = 0\n",
        "    # Process each page in the current date range.\n",
        "    for page in range(1, total_pages + 1):\n",
        "        params[\"page\"] = page\n",
        "        print(f\"  Processing page {page}/{total_pages} for range {gte} to {lte}...\")\n",
        "        try:\n",
        "            response = discover.movie(**params)\n",
        "        except Exception as e:\n",
        "            print(f\"  Error on page {page}: {e}\")\n",
        "            continue\n",
        "\n",
        "        movies = response.get(\"results\", [])\n",
        "        # Process movies in parallel using a ThreadPoolExecutor.\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "            futures = [executor.submit(process_movie, movie, main_writer, coll_writer,\n",
        "                                         genres_writer, prod_comp_writer, prod_countries_writer,\n",
        "                                         spoken_writer, lock)\n",
        "                       for movie in movies]\n",
        "            # Wait for all submitted tasks to complete.\n",
        "            concurrent.futures.wait(futures)\n",
        "\n",
        "        pages_processed += 1\n",
        "        # Flush file buffers after every 500 pages.\n",
        "        if pages_processed % 500 == 0:\n",
        "            for file_obj in flush_files.values():\n",
        "                file_obj.flush()\n",
        "            print(f\"  Processed {pages_processed} pages; file buffers flushed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7WjlSbsdjlH"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Main ETL Process\n",
        "# -----------------------------\n",
        "def main():\n",
        "    # Define output CSV filenames.\n",
        "    main_file = \"tmdb_movies_main.csv\"\n",
        "    collection_file = \"tmdb_movie_collection.csv\"\n",
        "    genres_file = \"tmdb_movie_genres.csv\"\n",
        "    production_companies_file = \"tmdb_movie_production_companies.csv\"\n",
        "    production_countries_file = \"tmdb_movie_production_countries.csv\"\n",
        "    spoken_languages_file = \"tmdb_movie_spoken_languages.csv\"\n",
        "\n",
        "    # Define CSV headers for each file.\n",
        "    main_headers = [\n",
        "        \"id\", \"adult\", \"backdrop_path\", \"budget\", \"homepage\", \"imdb_id\",\n",
        "        \"original_language\", \"original_title\", \"overview\", \"popularity\",\n",
        "        \"poster_path\", \"release_date\", \"revenue\", \"runtime\", \"status\",\n",
        "        \"tagline\", \"title\", \"video\", \"vote_average\", \"vote_count\"\n",
        "    ]\n",
        "    collection_headers = [\"movie_id\", \"collection_id\", \"collection_name\"]\n",
        "    genres_headers = [\"movie_id\", \"genre_id\", \"genre_name\"]\n",
        "    production_companies_headers = [\"movie_id\", \"company_id\", \"company_name\", \"origin_country\"]\n",
        "    production_countries_headers = [\"movie_id\", \"iso_3166_1\", \"country_name\"]\n",
        "    spoken_languages_headers = [\"movie_id\", \"iso_639_1\", \"language_name\"]\n",
        "\n",
        "    # Open CSV files for writing.\n",
        "    with open(main_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as main_csv, \\\n",
        "         open(collection_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as coll_csv, \\\n",
        "         open(genres_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as genres_csv, \\\n",
        "         open(production_companies_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as prod_comp_csv, \\\n",
        "         open(production_countries_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as prod_countries_csv, \\\n",
        "         open(spoken_languages_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as spoken_csv:\n",
        "\n",
        "        main_writer = csv.DictWriter(main_csv, fieldnames=main_headers)\n",
        "        coll_writer = csv.DictWriter(coll_csv, fieldnames=collection_headers)\n",
        "        genres_writer = csv.DictWriter(genres_csv, fieldnames=genres_headers)\n",
        "        prod_comp_writer = csv.DictWriter(prod_comp_csv, fieldnames=production_companies_headers)\n",
        "        prod_countries_writer = csv.DictWriter(prod_countries_csv, fieldnames=production_countries_headers)\n",
        "        spoken_writer = csv.DictWriter(spoken_csv, fieldnames=spoken_languages_headers)\n",
        "\n",
        "        # Write headers to each CSV file.\n",
        "        main_writer.writeheader()\n",
        "        coll_writer.writeheader()\n",
        "        genres_writer.writeheader()\n",
        "        prod_comp_writer.writeheader()\n",
        "        prod_countries_writer.writeheader()\n",
        "        spoken_writer.writeheader()\n",
        "\n",
        "        # Dictionary of file objects for flushing.\n",
        "        flush_files = {\n",
        "            \"main\": main_csv,\n",
        "            \"collection\": coll_csv,\n",
        "            \"genres\": genres_csv,\n",
        "            \"prod_companies\": prod_comp_csv,\n",
        "            \"prod_countries\": prod_countries_csv,\n",
        "            \"spoken\": spoken_csv\n",
        "        }\n",
        "\n",
        "        # Create a lock for thread-safe CSV writing.\n",
        "        lock = threading.Lock()\n",
        "\n",
        "        # Define the full date range: from 1900-01-01 to today.\n",
        "        start_date = \"1900-01-01\"\n",
        "        today_str = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n",
        "        process_date_range(start_date, today_str,\n",
        "                           main_writer, coll_writer, genres_writer,\n",
        "                           prod_comp_writer, prod_countries_writer, spoken_writer,\n",
        "                           flush_files, lock)\n",
        "\n",
        "        # Final flush after all processing is complete.\n",
        "        for file_obj in flush_files.values():\n",
        "            file_obj.flush()\n",
        "        print(f\"\\nCompleted processing movies from {start_date} to {today_str}. Files flushed to disk.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EXTRACTION ACTORS AND DIRECTORS:###"
      ],
      "metadata": {
        "id": "zWrj_Um1d2aG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHgn0v6KdjlI"
      },
      "outputs": [],
      "source": [
        "!pip install tmdbsimple\n",
        "import tmdbsimple as tmdb\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "tmdb.API_KEY = '73df6e10f160a8dc3afc422d2ddf7bb3'\n",
        "\n",
        "# Function to get actor/director details\n",
        "def get_person_details(person_id):\n",
        "    try:\n",
        "        person = tmdb.People(person_id)\n",
        "        details = person.info()\n",
        "        return {\n",
        "            'ID': person_id,\n",
        "            'Name': details.get('name', 'N/A'),\n",
        "            'Date of Birth': details.get('birthday', 'N/A'),\n",
        "            'Nationality': details.get('place_of_birth', 'N/A'),\n",
        "            'Known For': [job['title'] for job in details.get('known_for', [])],\n",
        "            'Biography': details.get('biography', 'N/A')\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching details for person ID {person_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to get actors and directors from popular movies\n",
        "def get_actors_directors_from_movies(year, num_movies=10):\n",
        "    movie_data = []\n",
        "    discover = tmdb.Discover()\n",
        "    response = discover.movie(primary_release_year=year, language='en')\n",
        "\n",
        "    for movie in response['results'][:num_movies]:  # Limit to avoid excessive requests\n",
        "        movie_id = movie['id']\n",
        "        credits = tmdb.Movies(movie_id).credits()\n",
        "\n",
        "        for person in credits.get('cast', [])[:5]:  # Get top 5 actors\n",
        "            movie_data.append({'Movie': movie['title'], 'Role': 'Actor', 'Person ID': person['id']})\n",
        "\n",
        "        for person in credits.get('crew', []):\n",
        "            if person['job'] == 'Director':\n",
        "                movie_data.append({'Movie': movie['title'], 'Role': 'Director', 'Person ID': person['id']})\n",
        "\n",
        "        time.sleep(0.5)  # Rate limit handling\n",
        "\n",
        "    return movie_data\n",
        "\n",
        "# Fetch actor & director data from movies of a given year\n",
        "movies_data = get_actors_directors_from_movies(2023, num_movies=5)\n",
        "\n",
        "# Get details for all actors/directors found\n",
        "people_data = []\n",
        "for entry in movies_data:\n",
        "    person_details = get_person_details(entry['Person ID'])\n",
        "    if person_details:\n",
        "        person_details['Movie'] = entry['Movie']\n",
        "        person_details['Role'] = entry['Role']\n",
        "        people_data.append(person_details)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_people = pd.DataFrame(people_data)\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df_people)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64w6lHhvdjlI"
      },
      "outputs": [],
      "source": [
        "!pip install tmdbsimple\n",
        "import tmdbsimple as tmdb\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "tmdb.API_KEY = '73df6e10f160a8dc3afc422d2ddf7bb3'\n",
        "\n",
        "# Function to get actor/director details\n",
        "def get_person_details(person_id):\n",
        "    \"\"\"Fetch details of an actor/director by ID\"\"\"\n",
        "    try:\n",
        "        person = tmdb.People(person_id)\n",
        "        details = person.info()\n",
        "        credits = person.combined_credits()  # Get movie & TV credits\n",
        "\n",
        "        return {\n",
        "            'ID': person_id,\n",
        "            'Name': details.get('name', 'N/A'),\n",
        "            'Date of Birth': details.get('birthday', 'N/A'),\n",
        "            'Nationality': details.get('place_of_birth', 'N/A'),\n",
        "            'Known For': [work['title'] for work in credits.get('cast', [])[:10]],  # Get more movies\n",
        "            'Biography': details.get('biography', 'N/A')\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching details for person ID {person_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to get actors & directors from a large set of movies\n",
        "def get_actors_directors_from_movies(start_year, end_year, max_movies_per_year=50):\n",
        "    \"\"\"Get a large list of actors and directors from many movies\"\"\"\n",
        "    movie_data = []\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        print(f\"Fetching movies from {year}...\")\n",
        "        discover = tmdb.Discover()\n",
        "        response = discover.movie(primary_release_year=year, language='en')\n",
        "\n",
        "        for movie in response['results'][:max_movies_per_year]:  # Get more movies per year\n",
        "            movie_id = movie['id']\n",
        "            credits = tmdb.Movies(movie_id).credits()\n",
        "\n",
        "            # Extract **all** actors\n",
        "            for person in credits.get('cast', []):\n",
        "                movie_data.append({'Movie': movie['title'], 'Role': 'Actor', 'Person ID': person['id']})\n",
        "\n",
        "            # Extract **all** directors\n",
        "            for person in credits.get('crew', []):\n",
        "                if person['job'] == 'Director':\n",
        "                    movie_data.append({'Movie': movie['title'], 'Role': 'Director', 'Person ID': person['id']})\n",
        "\n",
        "            time.sleep(0.2)  # Reduce delay for high-volume scraping\n",
        "\n",
        "    return movie_data\n",
        "\n",
        "# Function to scrape **thousands** of popular people\n",
        "def get_popular_people(max_pages=20):\n",
        "    \"\"\"Scrape as many popular actors & directors as possible\"\"\"\n",
        "    popular_people = []\n",
        "    people_api = tmdb.People()\n",
        "\n",
        "    for page in range(1, max_pages + 1):\n",
        "        print(f\"Fetching page {page} of popular people...\")\n",
        "        response = people_api.popular(page=page)\n",
        "\n",
        "        for person in response['results']:\n",
        "            popular_people.append({'Name': person['name'], 'Person ID': person['id']})\n",
        "\n",
        "        time.sleep(0.2)\n",
        "\n",
        "    return popular_people\n",
        "\n",
        "# Step 1: Scrape **movies from multiple years**\n",
        "movies_data = get_actors_directors_from_movies(2000, 2024, max_movies_per_year=100)  # Increase range for max data\n",
        "\n",
        "# Step 2: Scrape **thousands of popular people**\n",
        "popular_people = get_popular_people(max_pages=50)  # Increase pages for max data\n",
        "\n",
        "# Step 3: Get details for **all** actors & directors\n",
        "all_people_data = []\n",
        "\n",
        "# From **Movies**\n",
        "for entry in movies_data:\n",
        "    person_details = get_person_details(entry['Person ID'])\n",
        "    if person_details:\n",
        "        person_details['Movie'] = entry['Movie']\n",
        "        person_details['Role'] = entry['Role']\n",
        "        all_people_data.append(person_details)\n",
        "    time.sleep(0.1)  # Fast scraping\n",
        "\n",
        "# From **Popular People**\n",
        "for entry in popular_people:\n",
        "    person_details = get_person_details(entry['Person ID'])\n",
        "    if person_details:\n",
        "        person_details['Movie'] = 'N/A'  # No specific movie for popular people\n",
        "        person_details['Role'] = 'N/A'\n",
        "        all_people_data.append(person_details)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_people = pd.DataFrame(all_people_data)\n",
        "\n",
        "# Save to CSV for BIG data processing\n",
        "df_people.to_csv(\"actors_directors_data.csv\", index=False)\n",
        "\n",
        "# Display data\n",
        "from IPython.display import display\n",
        "display(df_people)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#extracting movies from known for column\n",
        "# Convert string representation of lists to actual lists using ast.literal_eval\n",
        "df_people['Known For'] = df['Known For'].apply(ast.literal_eval)\n",
        "\n",
        "# Create a new dataframe by exploding the Known For column\n",
        "new_df = df[['Name', 'Known For', 'Role']].explode('Known For')\n",
        "\n",
        "# Rename the columns\n",
        "new_df = new_df.rename(columns={'Known For': 'Movie/TV Show'})\n",
        "\n",
        "# Reset the index\n",
        "new_df = new_df.reset_index(drop=True)\n",
        "\n",
        "# Display the first 15 rows of the new dataframe\n",
        "print(\"New DataFrame (first 15 rows):\")\n",
        "print(new_df.head(15))\n",
        "\n",
        "# Display some basic statistics\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(f\"Total number of entries: {len(new_df)}\")\n",
        "print(f\"Number of unique people: {new_df['Name'].nunique()}\")\n",
        "print(f\"Number of unique movies/shows: {new_df['Movie/TV Show'].nunique()}\")\n",
        "\n",
        "# Save the new dataframe to a CSV file\n",
        "new_df.to_csv('person_movie_role.csv', index=False)\n",
        "print(\"\\nNew CSV file 'person_movie_role.csv' has been created!\")"
      ],
      "metadata": {
        "id": "BHhsx9PufgW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EXTRACTION TV SHOWS:####"
      ],
      "metadata": {
        "id": "wX9vMzcWfhf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showBasic(start_year, end_year):\n",
        "  shows_data = []\n",
        "  for year in range(start_year, end_year + 1):\n",
        "    print(f\"Fetching data for year {year}...\")\n",
        "    discover = tmdb.Discover()\n",
        "    response = discover.tv(first_air_date_year=year, language='en')\n",
        "\n",
        "    for show in response['results']:\n",
        "      shows_data.append({\n",
        "          'ID': show['id'],\n",
        "          'Name': show['name'],\n",
        "          'Original Name': show['original_name']\n",
        "          })\n",
        "\n",
        "    total_pages = response['total_pages']\n",
        "    for page in range(2, min(total_pages + 1, 501)):\n",
        "      response = discover.tv(first_air_date_year=year, page=page, language='en')\n",
        "      for show in response['results']:\n",
        "        shows_data.append({\n",
        "            'ID': show['id'],\n",
        "            'Name': show['name'],\n",
        "            'Original Name': show['original_name']\n",
        "            })\n",
        "\n",
        "    time.sleep(0.5)\n",
        "  return pd.DataFrame(shows_data)"
      ],
      "metadata": {
        "id": "aVOkG8gifo_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def showBasicDownload():\n",
        "  df1_1 = showBasic(start_year=1960, end_year=1970)\n",
        "  df1_2 = showBasic(start_year=1971, end_year=1980)\n",
        "  df1_3 = showBasic(start_year=1981, end_year=1990)\n",
        "  df1_4 = showBasic(start_year=1991, end_year=2000)\n",
        "  df1_5 = showBasic(start_year=2001, end_year=2010)\n",
        "  df1_6 = showBasic(start_year=2011, end_year=2015)\n",
        "  df1_7 = showBasic(start_year=2016, end_year=2019)\n",
        "  df1_8 = showBasic(start_year=2020, end_year=2022)\n",
        "  df1_9 = showBasic(start_year=2023, end_year=2025)\n",
        "\n",
        "  return pd.concat([df1_1, df1_2, df1_3, df1_4, df1_5, df1_6, df1_7, df1_8, df1_9], ignore_index=True)"
      ],
      "metadata": {
        "id": "n7P3T4S-f7Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = showBasicDownload()"
      ],
      "metadata": {
        "id": "xxERwKVVf7T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.to_csv('TV_Show(Basic).csv', index=False)"
      ],
      "metadata": {
        "id": "StreYf8wf7Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SHOW OVERVIEW ID SINOPSIS###"
      ],
      "metadata": {
        "id": "JxEohJ5Zg2Y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showOverview(start_year, end_year):\n",
        "  shows_data = []\n",
        "  for year in range(start_year, end_year + 1):\n",
        "    print(f\"Fetching data for year {year}...\")\n",
        "    discover = tmdb.Discover()\n",
        "    response = discover.tv(first_air_date_year=year, language='en')\n",
        "    for show in response['results']:\n",
        "      shows_data.append({\n",
        "          'ID': show['id'],\n",
        "          'Synopsis': show['overview']\n",
        "          })\n",
        "\n",
        "    total_pages = response['total_pages']\n",
        "    for page in range(2, min(total_pages + 1, 501)):\n",
        "      response = discover.tv(first_air_date_year=year, page=page, language='en')\n",
        "      for show in response['results']:\n",
        "        shows_data.append({\n",
        "            'ID': show['id'],\n",
        "            'Synopsis': show['overview']\n",
        "            })\n",
        "\n",
        "    time.sleep(0.5)\n",
        "  return pd.DataFrame(shows_data)"
      ],
      "metadata": {
        "id": "Yy1tnncqf7Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_1 = showOverview(start_year=1960, end_year=1970)\n",
        "  df1_2 = showOverview(start_year=1971, end_year=1980)\n",
        "  df1_3 = showOverview(start_year=1981, end_year=1990)\n",
        "  df1_4 = showOverview(start_year=1991, end_year=2000)\n",
        "  df1_5 = showOverview(start_year=2001, end_year=2010)\n",
        "  df1_6 = showOverview(start_year=2011, end_year=2015)\n",
        "  df1_7 = showOverview(start_year=2016, end_year=2019)\n",
        "  df1_8 = showOverview(start_year=2020, end_year=2022)\n",
        "  df1_9 = showOverview(start_year=2023, end_year=2025)\n",
        "\n",
        "  return pd.concat([df1_1, df1_2, df1_3, df1_4, df1_5, df1_6, df1_7, df1_8, df1_9], ignore_index=True)"
      ],
      "metadata": {
        "id": "2j5USvrTgdtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = showOverviewDownload()"
      ],
      "metadata": {
        "id": "tT1yeMjMgdmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.to_csv('TV_Show(Overview).csv', index=False)"
      ],
      "metadata": {
        "id": "egwIWU1ugddf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SHOWS SEASONS AND EPISODES ###"
      ],
      "metadata": {
        "id": "0PsgnRLIg9QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showSeasEps(start_year, end_year):\n",
        "    shows_data = []\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        print(f\"Fetching data for year {year}...\")\n",
        "\n",
        "        discover = tmdb.Discover()\n",
        "        response = discover.tv(first_air_date_year=year, language='en')\n",
        "\n",
        "        for show in response['results']:\n",
        "            try:\n",
        "                show_details = tmdb.TV(show['id']).info()\n",
        "                shows_data.append({\n",
        "                    'ID': show['id'],\n",
        "                    'Seasons': show_details.get('number_of_seasons', 0),\n",
        "                    'Episodes': show_details.get('number_of_episodes', 0)\n",
        "                })\n",
        "            except HTTPError as e:\n",
        "                print(f\"Error fetching data for show ID {show['id']}: {e}\")\n",
        "                shows_data.append({\n",
        "                    'ID': show['id'],\n",
        "                    'Seasons': \"Unknown (Error)\",\n",
        "                    'Episodes': \"Unknown (Error)\"\n",
        "                })\n",
        "\n",
        "        total_pages = response['total_pages']\n",
        "        for page in range(2, min(total_pages + 1, 501)):\n",
        "            response = discover.tv(first_air_date_year=year, page=page, language='en')\n",
        "            for show in response['results']:\n",
        "                try:\n",
        "                    show_details = tmdb.TV(show['id']).info()\n",
        "                    shows_data.append({\n",
        "                        'ID': show['id'],\n",
        "                        'Seasons': show_details.get('number_of_seasons', 0),\n",
        "                        'Episodes': show_details.get('number_of_episodes', 0)\n",
        "                    })\n",
        "                except HTTPError as e:\n",
        "                    print(f\"Error fetching data for show ID {show['id']}: {e}\")\n",
        "                    shows_data.append({\n",
        "                        'ID': show['id'],\n",
        "                        'Seasons': \"Unknown (Error)\",\n",
        "                        'Episodes': \"Unknown (Error)\"\n",
        "                    })\n",
        "\n",
        "        time.sleep(0.75)\n",
        "\n",
        "    return pd.DataFrame(shows_data)"
      ],
      "metadata": {
        "id": "LB8P5MPihDzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def showSeasEpsDownload():\n",
        "  df1_1 = showSeasEps(start_year=1960, end_year=1970)\n",
        "  df1_2 = showSeasEps(start_year=1971, end_year=1980)\n",
        "  df1_3 = showSeasEps(start_year=1981, end_year=1990)\n",
        "  df1_4 = showSeasEps(start_year=1991, end_year=2000)\n",
        "  df1_5 = showSeasEps(start_year=2001, end_year=2010)\n",
        "  df1_6 = showSeasEps(start_year=2011, end_year=2015)\n",
        "  df1_7 = showSeasEps(start_year=2016, end_year=2019)\n",
        "  df1_8 = showSeasEps(start_year=2020, end_year=2022)\n",
        "  df1_9 = showSeasEps(start_year=2023, end_year=2025)\n",
        "\n",
        "  return pd.concat([df1_1, df1_2, df1_3, df1_4, df1_5, df1_6, df1_7, df1_8, df1_9], ignore_index=True)"
      ],
      "metadata": {
        "id": "GnZ9guSdhEIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = showSeasEpsDownload()"
      ],
      "metadata": {
        "id": "4p6Fj2kjhEGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.to_csv('TV_Show(SeasEps).csv', index=False)"
      ],
      "metadata": {
        "id": "ivjhcrKkhEDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Show Production (ID, Exectuive Producers, Production Companies, Networks)###"
      ],
      "metadata": {
        "id": "S4zJN-TnhU7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showProduction(start_year, end_year):\n",
        "    shows_data = []\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        print(f\"Fetching data for year {year}...\")\n",
        "\n",
        "        discover = tmdb.Discover()\n",
        "        response = discover.tv(first_air_date_year=year, language='en')\n",
        "\n",
        "        for show in response['results']:\n",
        "            try:\n",
        "                show_details = tmdb.TV(show['id']).info()\n",
        "                credits = tmdb.TV(show['id']).credits()\n",
        "                executive_producers = [member['name'] for member in credits.get('crew', []) if member['job'] == 'Executive Producer']\n",
        "                production_companies = [company['name'] for company in show_details.get('production_companies', [])]\n",
        "                networks = [network['name'] for network in show_details.get('networks', [])]\n",
        "\n",
        "                shows_data.append({\n",
        "                    'ID': show['id'],\n",
        "                    'Executive Producers': executive_producers,\n",
        "                    'Production Companies': production_companies,\n",
        "                    'Networks': networks\n",
        "                })\n",
        "            except HTTPError as e:\n",
        "                print(f\"Error fetching data for show ID {show['id']}: {e}\")\n",
        "                shows_data.append({\n",
        "                    'ID': show['id'],\n",
        "                    'Executive Producers': [],\n",
        "                    'Production Companies': [],\n",
        "                    'Networks': []\n",
        "                })\n",
        "\n",
        "        total_pages = response['total_pages']\n",
        "        for page in range(2, min(total_pages + 1, 501)):\n",
        "            response = discover.tv(first_air_date_year=year, page=page, language='en')\n",
        "            for show in response['results']:\n",
        "                try:\n",
        "                    show_details = tmdb.TV(show['id']).info()\n",
        "                    credits = tmdb.TV(show['id']).credits()\n",
        "                    executive_producers = [member['name'] for member in credits.get('crew', []) if member['job'] == 'Executive Producer']\n",
        "                    production_companies = [company['name'] for company in show_details.get('production_companies', [])]\n",
        "                    networks = [network['name'] for network in show_details.get('networks', [])]\n",
        "                    shows_data.append({\n",
        "                        'ID': show['id'],\n",
        "                        'Executive Producers': executive_producers,\n",
        "                        'Production Companies': production_companies,\n",
        "                        'Networks': networks\n",
        "                    })\n",
        "                except HTTPError as e:\n",
        "                    print(f\"Error fetching data for show ID {show['id']}: {e}\")\n",
        "                    shows_data.append({\n",
        "                        'ID': show['id'],\n",
        "                        'Executive Producers': [],\n",
        "                        'Production Companies': [],\n",
        "                        'Networks': []\n",
        "                    })\n",
        "\n",
        "        time.sleep(0.75)\n",
        "\n",
        "    return pd.DataFrame(shows_data)"
      ],
      "metadata": {
        "id": "nk14KE1phEAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def showProductionDownload():\n",
        "  df1_1 = showProduction(start_year=1960, end_year=1970)\n",
        "  df1_2 = showProduction(start_year=1971, end_year=1980)\n",
        "  df1_3 = showProduction(start_year=1981, end_year=1990)\n",
        "  df1_4 = showProduction(start_year=1991, end_year=2000)\n",
        "  df1_5 = showProduction(start_year=2001, end_year=2010)\n",
        "  df1_6 = showProduction(start_year=2011, end_year=2015)\n",
        "  df1_7 = showProduction(start_year=2016, end_year=2019)\n",
        "  df1_8 = showProduction(start_year=2020, end_year=2022)\n",
        "  df1_9 = showProduction(start_year=2023, end_year=2025)\n",
        "\n",
        "  return pd.concat([df1_1, df1_2, df1_3, df1_4, df1_5, df1_6, df1_7, df1_8, df1_9], ignore_index=True)"
      ],
      "metadata": {
        "id": "Vbephy6PhD9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = showProductionDownload()"
      ],
      "metadata": {
        "id": "_x4iRDV3hh-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4.to_csv('TV_Show(Production).csv', index=False)"
      ],
      "metadata": {
        "id": "X6ZZiLQuhh42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Show Ratings (ID, Popularity Score, Genres)###"
      ],
      "metadata": {
        "id": "etMvh4wDhpLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showRatings(start_year, end_year):\n",
        "  shows_data = []\n",
        "  for year in range(start_year, end_year + 1):\n",
        "    print(f\"Fetching data for year {year}...\")\n",
        "    discover = tmdb.Discover()\n",
        "    response = discover.tv(first_air_date_year=year, language='en')\n",
        "\n",
        "    for show in response['results']:\n",
        "      show_id = show['id']\n",
        "      show_details = tmdb.TV(show_id)\n",
        "      show_details_info = show_details.info()\n",
        "      genres = [genre['name'] for genre in show_details_info.get('genres', [])]\n",
        "\n",
        "      shows_data.append({\n",
        "          'ID': show['id'],\n",
        "          'Popularity Score': show.get('popularity', 'Unknown'),\n",
        "          'Genres': ', '.join(genres)\n",
        "          })\n",
        "\n",
        "    total_pages = response['total_pages']\n",
        "    for page in range(2, min(total_pages + 1, 501)):\n",
        "      response = discover.tv(first_air_date_year=year, page=page, language='en')\n",
        "      for show in response['results']:\n",
        "        show_id = show['id']\n",
        "        show_details = tmdb.TV(show_id)\n",
        "        show_details_info = show_details.info()\n",
        "        genres = [genre['name'] for genre in show_details_info.get('genres', [])]\n",
        "\n",
        "        shows_data.append({\n",
        "            'ID': show['id'],\n",
        "            'Popularity Score': show.get('popularity', 'Unknown'),\n",
        "            'Genres': ', '.join(genres)\n",
        "            })\n",
        "\n",
        "    time.sleep(0.5)\n",
        "  return pd.DataFrame(shows_data)"
      ],
      "metadata": {
        "id": "MHKM7wBihhyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def showRatingsDownload():\n",
        "  df1_1 = showRatings(start_year=1960, end_year=1970)\n",
        "  df1_2 = showRatings(start_year=1971, end_year=1980)\n",
        "  df1_3 = showRatings(start_year=1981, end_year=1990)\n",
        "  df1_4 = showRatings(start_year=1991, end_year=2000)\n",
        "  df1_5 = showRatings(start_year=2001, end_year=2010)\n",
        "  df1_6 = showRatings(start_year=2011, end_year=2015)\n",
        "  df1_7 = showRatings(start_year=2016, end_year=2019)\n",
        "  df1_8 = showRatings(start_year=2020, end_year=2022)\n",
        "  df1_9 = showRatings(start_year=2023, end_year=2025)\n",
        "\n",
        "  return pd.concat([df1_1, df1_2, df1_3, df1_4, df1_5, df1_6, df1_7, df1_8, df1_9], ignore_index=True)"
      ],
      "metadata": {
        "id": "TrztligAh00m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5 = showRatingsDownload()"
      ],
      "metadata": {
        "id": "S535QFHph6t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5.to_csv('TV_Show(Ratings).csv', index=False)"
      ],
      "metadata": {
        "id": "wEJDSkxhh7Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Show Airings (ID, First Aired Date, Last Aired Date, Status)###"
      ],
      "metadata": {
        "id": "UJ1ptumUiDH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showAirings(start_year, end_year):\n",
        "  shows_data = []\n",
        "  for year in range(start_year, end_year + 1):\n",
        "    print(f\"Fetching data for year {year}...\")\n",
        "    discover = tmdb.Discover()\n",
        "    response = discover.tv(first_air_date_year=year, language='en')\n",
        "\n",
        "    for show in response['results']:\n",
        "      show_id = show['id']\n",
        "      show_details = tmdb.TV(show_id)\n",
        "      show_details_info = show_details.info()\n",
        "\n",
        "      shows_data.append({\n",
        "          'ID': show['id'],\n",
        "          'First Aired Date': show_details_info.get('first_air_date', 'Unknown'),\n",
        "          'Last Aired Date': show_details_info.get('last_air_date', 'Unknown'),\n",
        "          'Status': show_details_info.get('status', 'Unknown')\n",
        "          })\n",
        "\n",
        "    total_pages = response['total_pages']\n",
        "    for page in range(2, min(total_pages + 1, 501)):\n",
        "      response = discover.tv(first_air_date_year=year, page=page, language='en')\n",
        "      for show in response['results']:\n",
        "        show_id = show['id']\n",
        "        show_details = tmdb.TV(show_id)\n",
        "        show_details_info = show_details.info()\n",
        "\n",
        "        shows_data.append({\n",
        "            'ID': show['id'],\n",
        "            'First Aired Date': show_details_info.get('first_air_date', 'Unknown'),\n",
        "            'Last Aired Date': show_details_info.get('last_air_date', 'Unknown'),\n",
        "            'Status': show_details_info.get('status', 'Unknown')\n",
        "            })\n",
        "\n",
        "    time.sleep(0.75)\n",
        "  return pd.DataFrame(shows_data)"
      ],
      "metadata": {
        "id": "SiNqTa9bh7Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def showAiringsDownload():\n",
        "  df1_1 = showAirings(start_year=1960, end_year=1970)\n",
        "  df1_2 = showAirings(start_year=1971, end_year=1980)\n",
        "  df1_3 = showAirings(start_year=1981, end_year=1990)\n",
        "  df1_4 = showAirings(start_year=1991, end_year=2000)\n",
        "  df1_5 = showAirings(start_year=2001, end_year=2005)\n",
        "  df1_6 = showAirings(start_year=2006, end_year=2010)\n",
        "  df1_7 = showAirings(start_year=2011, end_year=2015)\n",
        "  df1_8 = showAirings(start_year=2016, end_year=2019)\n",
        "  df1_9 = showAirings(start_year=2020, end_year=2022)\n",
        "  df1_10 = showAirings(start_year=2023, end_year=2025)\n",
        "\n",
        "  return pd.concat([df1_1, df1_2, df1_3, df1_4, df1_5, df1_6, df1_7, df1_8, df1_9, df1_10], ignore_index=True)"
      ],
      "metadata": {
        "id": "jQJJFButiIgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df6 = showAiringsDownload()"
      ],
      "metadata": {
        "id": "gnucK_DSiMq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df6.to_csv('TV_Show(Airings).csv', index=False)"
      ],
      "metadata": {
        "id": "bsyV3_4AiIet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d8aZ8fEQiQAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DATA CLEANING TV SHOWS##\n"
      ],
      "metadata": {
        "id": "cx0pvsPwiQXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df7 = df4"
      ],
      "metadata": {
        "id": "0cIj5c6qiIcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_list_column(column):\n",
        "    return column.str.strip(\"[]\").str.replace(\"'\", \"\").str.replace('\"', '')"
      ],
      "metadata": {
        "id": "ES9X36LJiIZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_clean = [\"Executive Producers\", \"Production Companies\", \"Networks\"]\n",
        "df7[columns_to_clean] = df7[columns_to_clean].apply(clean_list_column)"
      ],
      "metadata": {
        "id": "FXmr6yDmifQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df8.to_csv('TV_Show(Production_Cleaned).csv', index=False)"
      ],
      "metadata": {
        "id": "cKNieSJhiIUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eO6zcNo3iIN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MOVIES and TV SHOWS EXTRACTION:"
      ],
      "metadata": {
        "id": "xk_-aEcJjZaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tmdbsimple as tmdb\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Your TMDb API key\n",
        "tmdb.API_KEY = '8db08b812689d86a7bf90611bf5a1eee'\n",
        "tmdb.REQUESTS_SESSION = requests.Session()\n",
        "\n",
        "\n",
        "# Function to inspect movie data (recommendations, reviews)\n",
        "def inspect_movie_data(movie_id):\n",
        "    movie = tmdb.Movies(movie_id)\n",
        "\n",
        "    # Fetch recommendations\n",
        "    recommendations = movie.recommendations()\n",
        "\n",
        "    # Extract movie recommendations data\n",
        "    movie_recommendations = []\n",
        "    for item in recommendations.get('results', []):\n",
        "        movie_recommendations.append({\n",
        "            'Movie ID': item.get('id'),\n",
        "            'Title': item.get('title'),\n",
        "            'Overview': item.get('overview'),\n",
        "            'Release Date': item.get('release_date'),\n",
        "            'Rating': item.get('vote_average')\n",
        "        })\n",
        "\n",
        "    # Fetch reviews\n",
        "    reviews = movie.reviews()\n",
        "\n",
        "    # Extract movie reviews data\n",
        "    movie_reviews = []\n",
        "    for item in reviews.get('results', []):\n",
        "        movie_reviews.append({\n",
        "            'Review ID': item.get('id'),\n",
        "            'Author': item.get('author'),\n",
        "            'Content': item.get('content'),\n",
        "            'Rating': item.get('author_details', {}).get('rating'),\n",
        "            'Review Date': item.get('created_at')\n",
        "        })\n",
        "\n",
        "    # Create DataFrames\n",
        "    recommendations_df = pd.DataFrame(movie_recommendations)\n",
        "    reviews_df = pd.DataFrame(movie_reviews)\n",
        "\n",
        "    return recommendations_df, reviews_df\n",
        "\n",
        "\n",
        "# Function to inspect TV series data (recommendations, reviews)\n",
        "def inspect_tv_series_data(tv_id):\n",
        "    tv_series = tmdb.TV(tv_id)\n",
        "\n",
        "    # Fetch recommendations\n",
        "    recommendations = tv_series.recommendations()\n",
        "\n",
        "    # Extract TV series recommendations data\n",
        "    tv_recommendations = []\n",
        "    for item in recommendations.get('results', []):\n",
        "        tv_recommendations.append({\n",
        "            'TV Series ID': item.get('id'),\n",
        "            'Title': item.get('name'),\n",
        "            'Overview': item.get('overview'),\n",
        "            'First Air Date': item.get('first_air_date'),\n",
        "            'Rating': item.get('vote_average')\n",
        "        })\n",
        "\n",
        "    # Fetch reviews\n",
        "    reviews = tv_series.reviews()\n",
        "\n",
        "    # Extract TV series reviews data\n",
        "    tv_reviews = []\n",
        "    for item in reviews.get('results', []):\n",
        "        tv_reviews.append({\n",
        "            'Review ID': item.get('id'),\n",
        "            'Author': item.get('author'),\n",
        "            'Content': item.get('content'),\n",
        "            'Rating': item.get('author_details', {}).get('rating'),\n",
        "            'Review Date': item.get('created_at')\n",
        "        })\n",
        "\n",
        "    # Create DataFrames\n",
        "    recommendations_df = pd.DataFrame(tv_recommendations)\n",
        "    reviews_df = pd.DataFrame(tv_reviews)\n",
        "\n",
        "    return recommendations_df, reviews_df\n",
        "\n",
        "\n",
        "\n",
        "def print_json_formatted(json_data):\n",
        "    \"\"\"Prints JSON data in a formatted way.\"\"\"\n",
        "    formatted_json = json.dumps(json_data, indent=4)\n",
        "    print(formatted_json)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Inspect Movie Data\n",
        "movie_recommendations_df, movie_reviews_df = inspect_movie_data(movie_id)\n",
        "print(\"Movie Recommendations:\")\n",
        "print(movie_recommendations_df.head())  # Display first 5 rows of movie recommendations\n",
        "print(\"\\nMovie Reviews:\")\n",
        "print(movie_reviews_df.head())  # Display first 5 rows of movie reviews\n",
        "\n",
        "# Inspect TV Series Data\n",
        "tv_recommendations_df, tv_reviews_df = inspect_tv_series_data(tv_id)\n",
        "print(\"\\nTV Series Recommendations:\")\n",
        "print(tv_recommendations_df.head())  # Display first 5 rows of TV series recommendations\n",
        "print(\"\\nTV Series Reviews:\")\n",
        "print(tv_reviews_df.head())  # Display first 5 rows of TV series reviews\n",
        "print(movie_recommendations_df.columns)\n",
        "# Save Movie DataFrames to CSV\n",
        "movie_recommendations_df.to_csv('movie_recommendations.csv', index=False)\n",
        "movie_reviews_df.to_csv('movie_reviews.csv', index=False)\n",
        "\n",
        "# Save TV Series DataFrames to CSV\n",
        "tv_recommendations_df.to_csv('tv_recommendations.csv', index=False)\n",
        "tv_reviews_df.to_csv('tv_reviews.csv', index=False)\n",
        "\n",
        "print(\"CSV files saved successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0IuctvqJjlQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}